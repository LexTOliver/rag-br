{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation: Embeddings and Indexing\n",
        "Este notebook refere-se à etapa de preparação dos dados, contemplando todo o pipeline de vetorização textual. Os passos aqui apresentados são:\n",
        "- Chunking dos documentos\n",
        "- Vetorização (embeddings)\n",
        "- Indexação e armazenamento vetorial\n",
        "Ao final, os dados de indexação são persistidos para um arquivo local.\n"
      ],
      "metadata": {
        "id": "hixY49SBOj6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalações e importações"
      ],
      "metadata": {
        "id": "DYZdVdLoRiyI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17cVm1hIOTnn"
      },
      "outputs": [],
      "source": [
        "# Instalações\n",
        "!pip install \"datasets<4.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "# Manipulação dos dados\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "# Embeddings\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "O7rQZPv4SM0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregamento dos dados"
      ],
      "metadata": {
        "id": "UzD8VT_gRlaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Quati dataset\n",
        "quati_ds = load_dataset(\"parquet\", data_files=\"./quati_processed.parquet\")\n",
        "\n",
        "print(quati_ds)\n",
        "print(\"-\" * 100)\n",
        "quati_ds[:5]"
      ],
      "metadata": {
        "id": "RixBJDjtRnWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking"
      ],
      "metadata": {
        "id": "hTCkgwZsRoV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
        "    device=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "id": "B9fUqQ0sRpYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_tokens=256, overlap=64):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens - overlap):\n",
        "        chunk = tokens[i:i + max_tokens]\n",
        "        chunks.append(tokenizer.decode(chunk))\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_by_tokens(text, chunk_size=384, overlap=64):\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "    input_ids = tokens[\"input_ids\"]\n",
        "    offsets = tokens[\"offset_mapping\"]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(input_ids):\n",
        "        end = start + chunk_size\n",
        "        chunk_offsets = offsets[start:end]\n",
        "        char_start = chunk_offsets[0][0]\n",
        "        char_end = chunk_offsets[-1][1]\n",
        "        chunks.append(text[char_start:char_end])\n",
        "        start += chunk_size - overlap\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "SE26n5-hWkpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying chunks into documents\n",
        "def apply_chunks(batch):\n",
        "    batch[\"chunks\"] = [chunk_by_tokens(doc) for doc in batch[\"passage\"]]\n",
        "    return batch\n",
        "\n",
        "quati_ds = quati_ds.map(\n",
        "    apply_chunks,\n",
        "    batched=True\n",
        "    batch_size=10_000,\n",
        "    num_proc=cpu_count\n",
        ")\n",
        "\n",
        "quati_ds.select_columns([\"chunks\", \"passage\"])[:2]"
      ],
      "metadata": {
        "id": "z1zLldXZXGOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstração das funções do Tokenizer\n",
        "\n",
        "Vamos usar a frase \"Olá, mundo! Como vai?\" para demonstrar as diferenças entre as funções do `tokenizer`."
      ],
      "metadata": {
        "id": "JkhzfRSnYNyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Olá, mundo! Como vai?\"\n",
        "\n",
        "print(f\"Texto original: {sample_text}\\n\")\n",
        "\n",
        "# 1. tokenizer(\"texto\") - Chamada direta\n",
        "print(\"--- tokenizer(\\\"texto\\\") ---\")\n",
        "encoded_full = tokenizer(sample_text, return_tensors='pt')\n",
        "print(\"Resultado (tensor):\", encoded_full)\n",
        "print(\"Input IDs (list):\", encoded_full['input_ids'].tolist())\n",
        "print(\"Attention Mask (list):\", encoded_full['attention_mask'].tolist())\n",
        "print(\"Decodificado de volta (para referência):\", tokenizer.decode(encoded_full['input_ids'][0]))\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. tokenizer.tokenize(\"texto\")\n",
        "print(\"--- tokenizer.tokenize(\\\"texto\\\") ---\")\n",
        "tokenized_strings = tokenizer.tokenize(sample_text)\n",
        "print(\"Resultado (tokens em string):\", tokenized_strings)\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. tokenizer.encode(\"texto\")\n",
        "print(\"--- tokenizer.encode(\\\"texto\\\") ---\")\n",
        "encoded_ids = tokenizer.encode(sample_text, add_special_tokens=True)\n",
        "print(\"Resultado (IDs numéricos):\", encoded_ids)\n",
        "print(\"Decodificado de volta (para referência):\", tokenizer.decode(encoded_ids))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Exemplo de tokenizer.encode com add_special_tokens=False (como na sua função chunk_text)\n",
        "print(\"--- tokenizer.encode(\\\"texto\\\", add_special_tokens=False) ---\")\n",
        "encoded_ids_no_special = tokenizer.encode(sample_text, add_special_tokens=False)\n",
        "print(\"Resultado (IDs numéricos sem tokens especiais):\", encoded_ids_no_special)\n",
        "print(\"Decodificado de volta (para referência):\", tokenizer.decode(encoded_ids_no_special))"
      ],
      "metadata": {
        "id": "V1pugzkIYJMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vetorização dos dados (embeddings)"
      ],
      "metadata": {
        "id": "nrfyrSbKRrOL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhqVIpZdR1EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexação"
      ],
      "metadata": {
        "id": "a_Nxv5w4R15B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDjn5G4ER4wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persistência dos dados"
      ],
      "metadata": {
        "id": "tDd_p3U9R5Op"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fq8RZDr5R8Ni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}